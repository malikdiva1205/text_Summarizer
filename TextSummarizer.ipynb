{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5AnwJBrDPPz",
        "outputId": "73622235-0acb-445d-97ea-3a4fdd19ef57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: rouge_score in /Users/user/Library/Python/3.9/lib/python/site-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /Users/user/Library/Python/3.9/lib/python/site-packages (from rouge_score) (2.1.0)\n",
            "Requirement already satisfied: nltk in /Users/user/Library/Python/3.9/lib/python/site-packages (from rouge_score) (3.2.4)\n",
            "Requirement already satisfied: numpy in /Users/user/Library/Python/3.9/lib/python/site-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from rouge_score) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install rouge_score # Install the missing package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIhLdNNJyTk-",
        "outputId": "00c684d1-5237-4b37-d609-486f112f7f4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: rouge in /Users/user/Library/Python/3.9/lib/python/site-packages (1.0.1)\n",
            "Requirement already satisfied: six in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from rouge) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install rouge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opo-O_2SjXAF",
        "outputId": "730910f5-e871-4a76-9889-3a260b27496a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/user/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /Users/user/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/user/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Step 1. Importing Libraries\n",
        "import sys\n",
        "import math\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import spacy\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "\n",
        "# Download 'wordnet' dataset\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "#Initializing few variable\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "#Initializing few variable\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjQxtkLMjcef",
        "outputId": "b6b70f9e-ed97-445d-9bda-7396c4879c37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "There is a massive disconnect between college students with innovative startup ideas and the resources they need to succeed â€” including expert mentorship, funding opportunities, and startup guidance. Currently, no centralized, student-friendly platform exists where mentors, investors, and founders can easily connect and collaborate in a trusted ecosystem.\n",
            "******************** Summary ********************\n",
            "\n",
            "\n",
            " Currently, no centralized, student-friendly platform exists where mentors, investors, and founders can easily connect and collaborate in a trusted ecosystem.\n",
            "\n",
            "\n",
            "\n",
            "Total words in original article =  38\n",
            "Total words in summarized article =  20\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "#Initializing few variable\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Step 3. Getting Text\n",
        "\n",
        "text = input(u\"Enter your text : \\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "#Step 4. Defining functions to create Tf-Idf Matrix\n",
        "\n",
        "\n",
        "#Function to calculate frequency of word in each sentence\n",
        "#INPUT -> List of all sentences from text as spacy.Doc object\n",
        "#OUTPUT -> freq_matrix (A dictionary with each sentence itself as key,\n",
        "# and a dictionary of words of that sentence with their frequency as value)\n",
        "\n",
        "def frequency_matrix(sentences):\n",
        "    freq_matrix = {}\n",
        "    stopWords = nlp.Defaults.stop_words\n",
        "\n",
        "    for sent in sentences:\n",
        "        freq_table = {} #dictionary with 'words' as key and their 'frequency' as value\n",
        "\n",
        "        #Getting all word from the sentence in lower case\n",
        "        words = [word.text.lower() for word in sent  if word.text.isalnum()]\n",
        "\n",
        "        for word in words:\n",
        "            word = lemmatizer.lemmatize(word)   #Lemmatize the word\n",
        "            if word not in stopWords:           #Reject stopwords\n",
        "                if word in freq_table:\n",
        "                    freq_table[word] += 1\n",
        "                else:\n",
        "                    freq_table[word] = 1\n",
        "\n",
        "        freq_matrix[sent[:15]] = freq_table\n",
        "\n",
        "    return freq_matrix\n",
        "\n",
        "\n",
        "#Function to calculate Term Frequency(TF) of each word\n",
        "#INPUT -> freq_matrix\n",
        "#OUTPUT -> tf_matrix (A dictionary with each sentence itself as key,\n",
        "# and a dictionary of words of that sentence with their Term-Frequency as value)\n",
        "\n",
        "#TF(t) = (Number of times term t appears in  document) / (Total number of terms in the document)\n",
        "def tf_matrix(freq_matrix):\n",
        "    tf_matrix = {}\n",
        "\n",
        "    for sent, freq_table in freq_matrix.items():\n",
        "        tf_table = {}  #dictionary with 'word' itself as a key and its TF as value\n",
        "\n",
        "        total_words_in_sentence = len(freq_table)\n",
        "        for word, count in freq_table.items():\n",
        "            tf_table[word] = count / total_words_in_sentence\n",
        "\n",
        "        tf_matrix[sent] = tf_table\n",
        "\n",
        "    return tf_matrix\n",
        "\n",
        "\n",
        "#Function to find how many sentences contain a 'word'\n",
        "#INPUT -> freq_matrix\n",
        "#OUTPUT -> sent_per_words (Dictionary with each word itself as key and number of\n",
        "#sentences containing that word as value)\n",
        "\n",
        "def sentences_per_words(freq_matrix):\n",
        "    sent_per_words = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        for word, count in f_table.items():\n",
        "            if word in sent_per_words:\n",
        "                sent_per_words[word] += 1\n",
        "            else:\n",
        "                sent_per_words[word] = 1\n",
        "\n",
        "    return sent_per_words\n",
        "\n",
        "\n",
        "#Function to calculate Inverse Document frequency(IDF) for each word\n",
        "#INPUT -> freq_matrix,sent_per_words, total_sentences\n",
        "#OUTPUT -> idf_matrix (A dictionary with each sentence itself as key,\n",
        "# and a dictionary of words of that sentence with their IDF as value)\n",
        "\n",
        "#IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
        "def idf_matrix(freq_matrix, sent_per_words, total_sentences):\n",
        "    idf_matrix = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        idf_table = {}\n",
        "\n",
        "        for word in f_table.keys():\n",
        "            idf_table[word] = math.log10(total_sentences / float(sent_per_words[word]))\n",
        "\n",
        "        idf_matrix[sent] = idf_table\n",
        "\n",
        "    return idf_matrix\n",
        "\n",
        "\n",
        "#Function to calculate Tf-Idf score of each word\n",
        "#INPUT -> tf_matrix, idf_matrix\n",
        "#OUTPUT - > tf_idf_matrix (A dictionary with each sentence itself as key,\n",
        "# and a dictionary of words of that sentence with their Tf-Idf as value)\n",
        "def tf_idf_matrix(tf_matrix, idf_matrix):\n",
        "    tf_idf_matrix = {}\n",
        "\n",
        "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
        "\n",
        "        tf_idf_table = {}\n",
        "\n",
        "       #word1 and word2 are same\n",
        "        for (word1, tf_value), (word2, idf_value) in zip(f_table1.items(),\n",
        "                                                    f_table2.items()):\n",
        "            tf_idf_table[word1] = float(tf_value * idf_value)\n",
        "\n",
        "        tf_idf_matrix[sent1] = tf_idf_table\n",
        "\n",
        "    return tf_idf_matrix\n",
        "\n",
        "\n",
        "#Function to rate every sentence with some score calculated on basis of Tf-Idf\n",
        "#INPUT -> tf_idf_matrix\n",
        "#OUTPUT - > sentenceScore (Dictionary with each sentence itself as key and its score\n",
        "# as value)\n",
        "def score_sentences(tf_idf_matrix):\n",
        "\n",
        "    sentenceScore = {}\n",
        "\n",
        "    for sent, f_table in tf_idf_matrix.items():\n",
        "        total_tfidf_score_per_sentence = 0\n",
        "\n",
        "        total_words_in_sentence = len(f_table)\n",
        "        for word, tf_idf_score in f_table.items():\n",
        "            total_tfidf_score_per_sentence += tf_idf_score\n",
        "\n",
        "        if total_words_in_sentence != 0:\n",
        "            sentenceScore[sent] = total_tfidf_score_per_sentence / total_words_in_sentence\n",
        "\n",
        "    return sentenceScore\n",
        "\n",
        "\n",
        "\n",
        "#Function Calculating average sentence score\n",
        "#INPUT -> sentence_score\n",
        "#OUTPUT -> average_sent_score(An average of the sentence_score)\n",
        "def average_score(sentence_score):\n",
        "\n",
        "    total_score = 0\n",
        "    for sent in sentence_score:\n",
        "        total_score += sentence_score[sent]\n",
        "\n",
        "    average_sent_score = (total_score / len(sentence_score))\n",
        "\n",
        "    return average_sent_score\n",
        "\n",
        "\n",
        "#Function to return summary of article\n",
        "#INPUT -> sentences(list of all sentences in article), sentence_score, threshold\n",
        "# (set to the average pf sentence_score)\n",
        "#OUTPUT -> summary (String text)\n",
        "def create_summary(sentences, sentence_score, threshold):\n",
        "    summary = ''\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sentence[:15] in sentence_score and sentence_score[sentence[:15]] >= (threshold):\n",
        "            summary += \" \" + sentence.text\n",
        "\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "#Step 5. Using all functions to generate summary\n",
        "\n",
        "\n",
        "#Counting number of words in original article\n",
        "original_words = text.split()\n",
        "original_words = [w for w in original_words if w.isalnum()]\n",
        "num_words_in_original_text = len(original_words)\n",
        "\n",
        "\n",
        "#Converting received text into sapcy Doc object\n",
        "text = nlp(text)\n",
        "\n",
        "#Extracting all sentences from the text in a list\n",
        "sentences = list(text.sents)\n",
        "total_sentences = len(sentences)\n",
        "\n",
        "#Generating Frequency Matrix\n",
        "freq_matrix = frequency_matrix(sentences)\n",
        "\n",
        "#Generating Term Frequency Matrix\n",
        "tf_matrix = tf_matrix(freq_matrix)\n",
        "\n",
        "#Getting number of sentences containing a particular word\n",
        "num_sent_per_words = sentences_per_words(freq_matrix)\n",
        "\n",
        "#Generating ID Frequency Matrix\n",
        "idf_matrix = idf_matrix(freq_matrix, num_sent_per_words, total_sentences)\n",
        "\n",
        "#Generating Tf-Idf Matrix\n",
        "tf_idf_matrix = tf_idf_matrix(tf_matrix, idf_matrix)\n",
        "\n",
        "\n",
        "#Generating Sentence score for each sentence\n",
        "sentence_scores = score_sentences(tf_idf_matrix)\n",
        "\n",
        "#Setting threshold to average value (You are free to play with ther values)\n",
        "threshold = average_score(sentence_scores)\n",
        "\n",
        "\n",
        "\n",
        "#Getting summary\n",
        "summary = create_summary(sentences, sentence_scores, 1 * threshold)\n",
        "print(\"\\n\\n\")\n",
        "print(text)\n",
        "print(\"*\"*20,\"Summary\",\"*\"*20)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(summary)\n",
        "print(\"\\n\\n\")\n",
        "print(\"Total words in original article = \", num_words_in_original_text)\n",
        "print(\"Total words in summarized article = \", len(summary.split()))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
